@@372514 In the mathematical discipline of linear algebra , the Schur decomposition or Schur triangulation , named after Issai Schur , is a matrix decomposition . # Statement # The Schur decomposition reads as follows : if ' ' A ' ' is a ' ' n ' ' &amp;times ; ' ' n ' ' square matrix with complex entries , then ' ' A ' ' can be expressed as : <math> A = Q U Q-1 </math> where ' ' Q ' ' is a unitary matrix ( so that its inverse ' ' Q ' ' <sup> 1 </sup> is also the conjugate transpose ' ' Q ' ' * of ' ' Q ' ' ) , and ' ' U ' ' is an upper triangular matrix , which is called a Schur form of ' ' A ' ' . Since ' ' U ' ' is similar to ' ' A ' ' , it has the same multiset of eigenvalues , and since it is triangular , those eigenvalues are the diagonal entries of ' ' U ' ' . @ @ @ @ @ @ @ @ @ @ of ' ' A ' ' -invariant subspaces 0 = ' ' V ' ' <sub> 0 </sub> ' ' V ' ' <sub> 1 </sub> .. ' ' V <sub> n </sub> ' ' = C <sup> ' ' n ' ' </sup> , and that there exists an ordered orthonormal basis ( for the standard Hermitian form of C <sup> ' ' n ' ' </sup> ) such that the first ' ' i ' ' basis vectors span ' ' V ' ' <sub> ' ' i ' ' </sub> for each ' ' i ' ' occurring in the nested sequence . Phrased somewhat differently , the first part says that a linear operator ' ' J ' ' on a complex finite-dimensional vector space stabilizes a complete flag ( ' ' V ' ' <sub> 1 </sub> , ... , ' ' V <sub> n </sub> ' ' ) . # Proof # A constructive proof for the Schur decomposition is as follows : every operator ' ' A ' ' on a complex finite-dimensional vector space has an eigenvalue ' ' &amp;lambda ; ' ' @ @ @ @ @ @ @ @ @ @ ; </sub> ' ' . Let ' ' V <sub> &amp;lambda ; </sub> ' ' <sup> </sup> be its orthogonal complement . It is clear that , with respect to this orthogonal decomposition , ' ' A ' ' has matrix representation ( one can pick here any orthonormal bases ' ' Z <sub> 1 </sub> ' ' and ' ' Z <sub> 2 </sub> ' ' spanning ' ' V <sub> &amp;lambda ; </sub> ' ' and ' ' V <sub> &amp;lambda ; </sub> ' ' <sup> </sup> respectively ) : <math> beginbmatrix Z1 &amp; Z2 endbmatrix* A beginbmatrixZ1 &amp; Z2endbmatrix = beginbmatrix lambda , Ilambda &amp; A12 0 &amp; A22 endbmatrix : beginmatrix Vlambda oplus Vlambdaperp endmatrix rightarrow beginmatrix Vlambda oplus Vlambdaperp endmatrix </math> where ' ' I <sub> &amp;lambda ; </sub> ' ' is the identity operator on ' ' V <sub> &amp;lambda ; </sub> ' ' . The above matrix would be upper-triangular except for the ' ' A ' ' <sub> 22 </sub> block . But exactly the same procedure can be applied to the sub-matrix ' ' A ' ' <sub> 22 </sub> , viewed @ @ @ @ @ @ @ @ @ @ </sub> ' ' <sup> </sup> , and its submatrices . Continue this way n times . Thus the space C <sup> ' ' n ' ' </sup> will be exhausted and the procedure has yielded the desired result . The above argument can be slightly restated as follows : let ' ' &amp;lambda ; ' ' be an eigenvalue of ' ' A ' ' , corresponding to some eigenspace ' ' V <sub> &amp;lambda ; </sub> ' ' . ' ' A ' ' induces an operator ' ' T ' ' on the quotient space C <sup> ' ' n ' ' </sup> modulo ' ' V <sub> &amp;lambda ; </sub> ' ' . This operator is precisely the ' ' A ' ' <sub> 22 </sub> submatrix from above . As before , ' ' T ' ' would have an eigenspace , say ' ' W <sub> &amp;mu ; </sub> ' ' C <sup> ' ' n ' ' </sup> modulo ' ' V <sub> &amp;lambda ; </sub> ' ' . Notice the preimage of ' ' W <sub> &amp;mu ; </sub> ' ' under the quotient @ @ @ @ @ @ @ @ @ @ ' that contains ' ' V <sub> &amp;lambda ; </sub> ' ' . Continue this way until the resulting quotient space has dimension 0 . Then the successive preimages of the eigenspaces found at each step form a flag that ' ' A ' ' stabilizes. # Notes # Although every square matrix has a Schur decomposition , in general this decomposition is not unique . For example , the eigenspace ' ' V <sub> &amp;lambda ; </sub> ' ' can have dimension 1 , in which case any orthonormal basis for ' ' V <sub> &amp;lambda ; </sub> ' ' would lead to the desired result . Write the triangular matrix ' ' U ' ' as ' ' U ' ' = ' ' D ' ' + ' ' N ' ' , where ' ' D ' ' is diagonal and ' ' N ' ' is strictly upper triangular ( and thus a nilpotent matrix ) . The diagonal matrix ' ' D ' ' contains the eigenvalues of ' ' A ' ' in arbitrary order ( hence its Frobenius norm , squared , is @ @ @ @ @ @ @ @ @ @ ' ' A ' ' , while the Frobenius norm of ' ' A ' ' , squared , is the sum of the squared singular values of ' ' A ' ' ) . The nilpotent part ' ' N ' ' is generally not unique either , but its Frobenius norm is uniquely determined by ' ' A ' ' ( just because the Frobenius norm of A is equal to the Frobenius norm of ' ' U ' ' = ' ' D ' ' + ' ' N ' ' ) . It is clear that if ' ' A ' ' is a normal matrix , then ' ' U ' ' from its Schur decomposition must be a diagonal matrix and the column vectors of ' ' Q ' ' are the eigenvectors of ' ' A ' ' . Therefore , the Schur decomposition extends the spectral decomposition . In particular , if ' ' A ' ' is positive definite , the Schur decomposition of ' ' A ' ' , its spectral decomposition , and its singular value decomposition coincide . A @ @ @ @ @ @ @ @ @ @ of matrices can be simultaneously triangularized , i.e. there exists a unitary matrix ' ' Q ' ' such that , for every ' ' A <sub> i </sub> ' ' in the given family , ' ' Q A <sub> i </sub> Q* ' ' is upper triangular . This can be readily deduced from the above proof . Take element ' ' A ' ' from ' ' A <sub> i </sub> ' ' and again consider an eigenspace ' ' V <sub> A </sub> ' ' . Then ' ' V <sub> A </sub> ' ' is invariant under all matrices in ' ' A <sub> i </sub> ' ' . Therefore all matrices in ' ' A <sub> i </sub> ' ' must share one common eigenvector in ' ' V <sub> A </sub> ' ' . Induction then proves the claim . As a corollary , we have that every commuting family of normal matrices can be simultaneously diagonalized . In the infinite dimensional setting , not every bounded operator on a Banach space has an invariant subspace . However , the upper-triangularization of an arbitrary @ @ @ @ @ @ @ @ @ @ operator on a complex Banach space has a nest of closed invariant subspaces. # Computation # Schur decomposition of a given matrix is known to be numerically computed by QR algorithm or its variants . In other words , the roots of the characteristic polynomial corresponding to the matrix are not necessarily computed ahead in order to obtain its Schur decomposition . Conversely , QR algorithm can be used to compute the roots of any given characteristic polynomial by finding the Schur decomposition of its companion matrix . Similarly , QR algorithm is used to compute the eigenvalues of any given matrix , which are the diagonal entries of the upper triangular matrix of the Schur decomposition . See the Nonsymmetric Eigenproblems section in LAPACK Users ' Guide . # Applications # Lie theory applications include : * Every invertible operator is contained in a Borel group . * Every operator fixes a point of the flag manifold . # Generalized Schur decomposition # Given square matrices ' ' A ' ' and ' ' B ' ' , the generalized Schur decomposition factorizes both matrices as <math> A=QSZ* </math> and @ @ @ @ @ @ @ @ @ @ and ' ' Z ' ' are unitary , and ' ' S ' ' and ' ' T ' ' are upper triangular . The generalized Schur decomposition is also sometimes called the QZ decomposition . The generalized eigenvalues <math> lambda </math> that solve the generalized eigenvalue problem <math> Ax=lambda Bx </math> ( where ' ' x ' ' is an unknown nonzero vector ) can be calculated as the ratio of the diagonal elements of ' ' S ' ' to those of ' ' T ' ' . That is , using subscripts to denote matrix elements , the ' ' i ' ' th generalized eigenvalue <math> lambdai </math> satisfies <math> lambdai=Sii/Tii </math> . # References # **14;362474;references 
