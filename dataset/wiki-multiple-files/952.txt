@@4882514 The Remez algorithm or Remez exchange algorithm , published by Evgeny Yakovlevich Remez in 1934 , is an iterative algorithm used to find simple approximations to functions , specifically , approximations by functions in a Chebyshev space that are the best in the uniform norm ' ' L ' ' <sub> </sub> sense . A typical example of a Chebyshev space is the subspace of Chebyshev polynomials of order ' ' n ' ' in the space of real continuous functions on an interval , ' ' C ' ' ' ' a ' ' , ' ' b ' ' . The polynomial of best approximation within a given subspace is defined to be the one that minimizes the maximum absolute difference between the polynomial and the function . In this case , the form of the solution is precised by the equioscillation theorem . # Procedure # The Remez algorithm starts with the function ' ' f ' ' to be approximated and a set ' ' X ' ' of <math> n + 2 </math> sample points <math> x1 , x2 , ... , xn+2 @ @ @ @ @ @ @ @ @ @ linearly mapped to the interval . The steps are : # Solve the linear system of equations : <math> b0 + b1 xi+ .. +bn xi n + ( -1 ) i E = f(xi) </math> ( where <math> i=1 , 2 , .. n+2 </math> ) , : for the unknowns <math> b0 , b1 ... bn </math> and ' ' E ' ' . # Use the <math> bi </math> as coefficients to form a polynomial <math> Pn </math> . # Find the set ' ' M ' ' of points of local maximum error <math> Pn(x) - f(x) </math> . # If the errors at every <math> m in M </math> are of equal magnitude and alternate in sign , then <math> Pn </math> is the minimax approximation polynomial . If not , replace ' ' X ' ' with ' ' M ' ' and repeat the steps above . The result is called the polynomial of best approximation , the Chebyshev approximation , or the minimax approximation . A review of technicalities in implementing the Remez algorithm is given by W. Fraser . # On the @ @ @ @ @ @ @ @ @ @ choice for the initial approximation because of their role in the theory of polynomial interpolation . For the initialization of the optimization problem for function ' ' f ' ' by the Lagrange interpolant ' ' L ' ' <sub> n </sub> ( ' ' f ' ' ) , it can be shown that this initial approximation is bounded by : <math> lVert f - Ln(f)rVertinfty le ( 1 + lVert LnrVertinfty ) infp in Pn lVert f - prVert </math> with the norm or Lebesgue constant of the Lagrange interpolation operator ' ' L ' ' <sub> ' ' n ' ' </sub> of the nodes ( ' ' t ' ' <sub> 1 </sub> , ... , ' ' t ' ' <sub> ' ' n ' ' + 1 </sub> ) being : <math> lVert LnrVertinfty = overlineLambdan(T) = max-1 le x le 1 lambdan ( T ; x ) , </math> ' ' T ' ' being the zeros of the Chebyshev polynomials , and the Lebesgue functions being : <math> lambdan ( T ; x ) = sumj = 1n + 1 left lj(x) right @ @ @ @ @ @ @ @ @ @ 1 frac ( x - ti ) ( tj - ti ) . </math> Theodore A. Kilgore , Carl de Boor , and Allan Pinkus proved that there exists a unique ' ' t ' ' <sub> ' ' i ' ' </sub> for each ' ' L ' ' <sub> ' ' n ' ' </sub> , although not known explicitly for ( ordinary ) polynomials . Similarly , <math> underlineLambdan(T) = min-1 le x le 1 lambdan ( T ; x ) </math> , and the optimality of a choice of nodes can be expressed as <math> overlineLambdan - underlineLambdan ge 0 . </math> For Chebyshev nodes , which provides a suboptimal , but analytically explicit choice , the asymptotic behavior is known as : <math> overlineLambdan(T) = frac2pi log ( n + 1 ) + frac2pileft ( gamma + logfrac8piright ) + alphan + 1 </math> ( ' ' ' ' being the Euler-Mascheroni constant ) with : <math> 0 **38;224707; for <math> n ge 1 , </math> and upper bound : <math> overlineLambdan(T) le frac2pi log ( n + 1 ) + 1 </math> Lev Brutman obtained @ @ @ @ @ @ @ @ @ @ <math> hatT </math> being the zeros of the expanded Chebyshev polynomials : : <math> overlineLambdan(hatT) - underlineLambdan(hatT) **153;224747; Rdiger Gnttner obtained from a sharper estimate for <math> n ge 40 </math> : <math> overlineLambdan(hatT) - underlineLambdan(hatT) **16;224902; # Detailed Discussion # Here we provide more information on the steps outlined above . In this section we let the index ' ' i ' ' run from 0 to ' ' n ' ' +1 . Step 1 : Given <math> x0 , x1 , .. xn+1 </math> , solve the linear system of ' ' n ' ' +2 equations : <math> b0 + b1 xi+ .. +bn xi n + ( -1 ) i E = f(xi) </math> ( where <math> i=0 , 1 , .. n+1 </math> ) , : for the unknowns <math> b0 , b1 , ... bn </math> and ' ' E ' ' . It should be clear that <math> ( -1 ) i E </math> in this equation makes sense only if the nodes <math> x0 , ... , xn+1 </math> are ' ' ordered ' ' , either strictly increasing or strictly decreasing @ @ @ @ @ @ @ @ @ @ ( As is well known , not every linear system has a solution . ) Also , the solution can be obtained with only <math> O(n2) </math> arithmetic operations while a standard solver from the library would take <math> O(n3) </math> operations . Here is the simple proof : Compute the standard ' ' n ' ' -th degree interpolant <math> p1(x) </math> to <math> f(x) </math> at the first ' ' n ' ' +1 nodes and also the standard ' ' n ' ' -th degree interpolant <math> p2(x) </math> to the ordinates <math> ( -1 ) i </math> : <math> p1(xi) = f(xi) , p2(xi) = ( -1 ) i , i = 0 , ... , n . </math> To this end use each time Newton 's interpolation formula with the divided differences of order <math> 0 , ... , n </math> and <math> O(n2) </math> arithmetic operations . The polynomial <math> p2(x) </math> has its ' ' i ' ' -th zero between <math> xi-1 </math> and <math> xi , i=1 , ... , n </math> , and thus no further zeroes between <math> xn </math> @ @ @ @ @ @ @ @ @ @ p2(xn+1) </math> have the same sign <math> ( -1 ) n </math> . The linear combination <math> p(x) : = p1 ( x ) - p2(x) ! cdot ! E </math> is also a polynomial of degree ' ' n ' ' and : <math> p(xi) = p1(xi) - p2(xi) ! cdot ! E = f(xi) - ( -1 ) i E , i =0 , ldots , n . </math> This is the same as the equation above for <math> i = 0 , .. , n </math> and for any choice of ' ' E ' ' . The same equation for ' ' i ' ' = ' ' n ' ' +1 is : <math> p(xn+1) = p1(xn+1) - p2(xn+1) ! cdot ! E = f(xn+1) - ( -1 ) n+1 E </math> and needs special reasoning : solved for the variable ' ' E ' ' , it is the ' ' definition ' ' of ' ' E ' ' : : <math> E : = fracp1(xn+1) - f(xn+1)p2(xn+1) + ( -1 ) n . </math> As mentioned above , the two terms in the @ @ @ @ @ @ @ @ @ @ and thus <math> p(x) equiv b0 + b1x + ldots + bnxn </math> are always well-defined . The error at the given ' ' n ' ' +2 ordered nodes is positive and negative in turn because : <math> p(xi) - f(xi) = -(-1)i E , i = 0 , .. , n ! + ! 1 . </math> The Theorem of ' ' de La Valle Poussin ' ' states that under this condition no polynomial of degree ' ' n ' ' exists with error less than ' ' E ' ' . Indeed , if such a polynomial existed , call it <math> tilde p(x) </math> , then the difference <math> p(x)-tilde p(x) = ( p(x) - f(x) - ( tilde p(x) - f(x) </math> would still be positive/negative at the ' ' n ' ' +2 nodes <math> xi </math> and therefore have at least ' ' n ' ' +1 zeros which is impossible for a polynomial of degree ' ' n ' ' . Thus , this ' ' E ' ' is a lower bound for the minimum error which can be achieved with @ @ @ @ @ @ @ @ @ @ 2 changes the notation from <math> b0 + b1x + .. + bnxn </math> to <math> p(x) </math> . Step 3 improves upon the input nodes <math> x0 , ... , xn+1 </math> and their errors <math> pm E </math> as follows . In each P-region , the current node <math> xi </math> is replaced with the local maximizer <math> barxi </math> and in each N-region <math> xi </math> is replaced with the local minimizer . ( Expect <math> barx0 </math> at ' ' A ' ' , the <math> bar xi </math> near <math> xi </math> , and <math> barxn+1 </math> at ' ' B ' ' . ) No high precision is required here , the standard ' ' line search ' ' with a couple of ' ' quadratic fits ' ' should suffice . ( See ) Let <math> zi : = p(barxi) - f(barxi) </math> . Each amplitude <math> zi </math> is greater than or equal to ' ' E ' ' . The Theorem of ' ' de La Valle Poussin ' ' and its proof also apply to <math> z0 , .. , zn+1 @ @ @ @ @ @ @ @ @ @ lower bound for the best error possible with polynomials of degree ' ' n ' ' . Moreover , <math> maxzi </math> comes in handy as an obvious upper bound for that best possible error . Step 4 : With <math> min , zi </math> and <math> max , zi </math> as lower and upper bound for the best possible approximation error , one has a reliable stopping criterion : repeat the steps until <math> maxzi - minzi </math> is sufficiently small or no longer decreases . These bounds indicate the progress . # Variants # Sometimes more than one sample point is replaced at the same time with the locations of nearby maximum absolute differences . Sometimes relative error is used to measure the difference between the approximation and the function , especially if the approximation will be used to compute the function on a computer which uses floating point arithmetic . 
